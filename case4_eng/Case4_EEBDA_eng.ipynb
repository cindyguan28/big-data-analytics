{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'EEBDA is fun!',\n",
    "    'EEBDA is a great course.',\n",
    "    'house',\n",
    "    'House',\n",
    "    'Houses'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 11) # the maximum number of words to keep, based on word frequency\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\") # load english model\n",
    "\n",
    "document = nlp(\"EEBDA is SO much fun!\")\n",
    "\n",
    "pd.DataFrame({\"Token\": [word.text for word in document],\n",
    "              \"Base\": [word.lemma_ for word in document]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nlp(\"dog\")\n",
    "    \n",
    "embed.vector[0:10] # show first 10 entries for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"dog\")\n",
    "doc2 = nlp(\"cat\")\n",
    "\n",
    "# Similarity of two words\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_csv(\".\\\\case4_train.csv\", header=None, nrows= 1000)\n",
    "df_test = pd.read_csv(\".\\\\case4_test.csv\", header=None)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install farm\n",
    "\n",
    "import torch # machine learning \"framework\"\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import TextClassificationHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.train import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loading tokenizer\n",
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=\"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# set labels corresponding to \"negative\", \"neutral\" and \"positive\"\n",
    "LABEL_LIST = [\"0\", \"2\", \"4\"]\n",
    "\n",
    "# define the classification task\n",
    "processor = TextClassificationProcessor(tokenizer=tokenizer, # here: \"bert-base-uncased\"\n",
    "                                        max_seq_len=128, # samples are truncated after this many tokens\n",
    "                                        data_dir=r\".\\\\\", # stored tweets\n",
    "                                        label_list=LABEL_LIST, # \"0\", \"2\", \"4\"\n",
    "                                        metric=\"f1_macro\", # used for evaluation\n",
    "                                        label_column_name=\"lable\") # column with training lables(0,2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 # batch size is a number of samples processed before the model is updated\n",
    "\n",
    "data_silo = DataSilo( # generates and stores PyTorch DataLoader objects for the train, dev, and test datasets.\n",
    "    processor=processor, # see above\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "language_model = LanguageModel.load(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_head = TextClassificationHead(num_labels=len(LABEL_LIST))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDS_DROPOUT_PROB = 0.1 # The probability that a value in the embeddings returned by the language model will be zeroed.\n",
    "                          # Helps preventing the model from overfitting!\n",
    "\n",
    "device = \"cpu\" # using cpu since not everybody has a gpu installed\n",
    "\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[prediction_head],\n",
    "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
    "    lm_output_types=[\"per_sequence\"], #  How to extract the embeddings from the final layer of the language model.\n",
    "                                      # If set to “per_sequence”, a single embedding will be extracted to represent \n",
    "                                      # the full input sequence.\n",
    "    device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LEARNING_RATE = 2e-5 # determins step size at each iteration while moving to minimum of a loss function\n",
    "N_EPOCHS = 1 # determins number of passes of the entire training dataset the algorithm has to complete\n",
    "\n",
    "\n",
    "model, optimizer, lr_schedule = initialize_optimizer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_batches=len(data_silo.loaders[\"train\"]),\n",
    "    n_epochs=N_EPOCHS) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_GPU = 0 # set zero since we are not using any gpu\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_silo=data_silo,\n",
    "    epochs=N_EPOCHS,\n",
    "    n_gpu=N_GPU,\n",
    "    lr_schedule=lr_schedule,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farm.infer import Inferencer\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "infer_model = Inferencer(processor=processor, model=model, task_type=\"text_classification\", gpu=False)\n",
    "\n",
    "basic_texts = [\n",
    "    {\"text\": \"EEBDA is such a great course!\"},\n",
    "]\n",
    "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
    "PrettyPrinter().pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
